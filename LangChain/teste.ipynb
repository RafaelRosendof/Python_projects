{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/anaconda3/envs/figas2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/rafael/anaconda3/envs/figas2/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:961: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers , torch\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "# begin initializing HF items, you need an access token\n",
    "hf_auth = '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'\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/anaconda3/envs/figas2/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/anaconda3/envs/figas2/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([    1, 29871,    13, 29950,  7889, 29901], device='cuda:0'),\n",
       " tensor([    1, 29871,    13, 28956,    13], device='cuda:0')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "# enable evaluation mode to allow model inference\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "\n",
    "stop_list = ['\\nHuman:', '\\n```\\n']\n",
    "\n",
    "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "stop_token_ids\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how can i'm use llama with the langchain?.\n",
      " everybody is talking about it but i don't know where to start. Can you give me some tips on how to get started with Llama and Langchain?\n",
      "\n",
      "Llama is a tool for building, deploying, and managing serverless applications. It provides a simple and intuitive way to create and manage functions, APIs, and other serverless resources in popular cloud providers like AWS, Google Cloud, and Azure.\n",
      "\n",
      "Here are some steps to help you get started with Llama and Langchain:\n",
      "\n",
      "1. Install Llama:\n",
      "\t* You can install Llama using npm by running `npm install -g llama`.\n",
      "\t* Alternatively, you can download the binary from the Llama website and install it manually.\n",
      "2. Create a new project:\n",
      "\t* Once installed, open a terminal or command prompt and run `llama init my-project`.\n",
      "\t* This will create a new directory called `my-project` containing the basic structure for a Llama project.\n",
      "3. Write your code:\n",
      "\t* In the `my-project` directory, create a new file called `main.js` (or `main.ts` for TypeScript projects).\n",
      "\t* Write your code in this file, using the Llama syntax for defining functions and other serverless resources.\n",
      "4. Deploy your code:\n",
      "\t* Once you have written your code, you can deploy it to your preferred cloud provider using `llama deploy`.\n",
      "\t* For example, to deploy your code to AWS, you can run `llama deploy --provider aws`.\n",
      "5. Test your code:\n",
      "\t* After deploying your code, you can test it using `llama test`.\n",
      "\t* This will run your tests and report any errors or failures.\n",
      "6. Monitor your code:\n",
      "\t* To monitor your code in production, you can use `llama monitor`.\n",
      "\t* This will show you real-time metrics and logs for your serverless application.\n",
      "7. Learn more:\n",
      "\t* Check out the Llama documentation for more information on how to use the tool.\n",
      "\t* There are also many tutorials and guides available online that can help you learn more about Llama and serverless development.\n",
      "\n",
      "By following these steps, you should be able to get started with Llama and Langchain and start building your own serverless applications.\n"
     ]
    }
   ],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n",
    "\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")\n",
    "\n",
    "res = generate_text(\"how can i'm use llama with the langchain?.\")\n",
    "print(res[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain\n",
    "\n",
    "Iniciando o uso do HF pipeline com o LangChain \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2217713/935060849.py:3: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  llm = HuggingFacePipeline(pipeline=generate_text)\n",
      "/tmp/ipykernel_2217713/935060849.py:5: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  llm(prompt=\"how can i'm use llama with the langchain?.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"how can i'm use llama with the langchain?.\\n everybody is talking about it but i don't know where to start. Can you give me some tips on how to get started with llama and the language chain?\\n\\nLlama is a powerful tool for building, deploying, and managing serverless applications. Here are some tips on how to get started with Llama and the Language Chain:\\n\\n1. Familiarize yourself with the basics of serverless computing: Before diving into Llama, it's essential to understand the basics of serverless computing. Learn about the benefits of serverless architecture, such as scalability, cost-effectiveness, and reduced maintenance burden.\\n2. Install Llama: To use Llama, you need to install it in your local environment. You can do this by running the following command in your terminal or command prompt:\\n```\\nnpm install -g @llama/cli\\n```\\n3. Create a new project: Once Llama is installed, you can create a new project by running the following command:\\n```\\nllama init my-project\\n```\\nThis will create a new directory called `my-project` containing the basic structure for a Llama project.\\n4. Write your code: The next step is to write your code using any programming language supported by Llama. Some popular choices include Node.js, Python, and Go.\\n5. Build and deploy your application: Once you have written your code, you can build and deploy your application using Llama. You can run the following commands to build and deploy your application:\\n```\\nllama build\\nllama deploy\\n```\\nThese commands will compile your code, package it into a container image, and deploy it to your chosen cloud provider (AWS, Google Cloud, or Azure).\\n6. Use the Language Chain: The Language Chain is a set of tools that allow you to write code in one language and deploy it to another language. For example, you can write your backend code in Node.js and deploy it to a frontend built with React. To use the Language Chain, you need to install the required packages for each language you want to use.\\n7. Connect your languages: Once you have installed the necessary packages, you can connect your languages using Llama. This allows you to write code in one language and deploy it to another without having to worry about the underlying infrastructure.\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "llm(prompt=\"how can i'm use llama with the langchain?.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iniciando o uso de uma leitura de documentos\n",
    "\n",
    "Segue o código a baixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 1168 0 (offset 0)\n",
      "Ignoring wrong pointing object 1702 0 (offset 0)\n",
      "Ignoring wrong pointing object 12853 0 (offset 0)\n",
      "Ignoring wrong pointing object 20965 0 (offset 0)\n",
      "Ignoring wrong pointing object 29012 0 (offset 0)\n",
      "Ignoring wrong pointing object 30191 0 (offset 0)\n",
      "Ignoring wrong pointing object 32043 0 (offset 0)\n",
      "Ignoring wrong pointing object 55601 0 (offset 0)\n",
      "Ignoring wrong pointing object 63890 0 (offset 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '/home/rafael/Desktop/LFA/LFA2.pdf', 'page': 1}, page_content='This is an electronic version of the print textbook. Due to electronic rights restrictions, some third party content \\nmay be suppressed. Editorial review has deemed that any suppressed content does not materially affect the overall \\nlearning experience. The publisher reserves the right to remove content from this title at any time if subsequent \\nrights restrictions require it. For valuable information on pricing, previous editions, changes to\\ncurrent editions, and alternate formats, please visit www.cengage.com/highered to search by\\nISBN#, author, title, or keyword for materials in your areas of interest .\\nCopyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = (\"/home/rafael/Desktop/LFA/LFA2.pdf\")\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "pages[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load WEB\n",
    "carregando arquivos da web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "web_links = [\"https://www.databricks.com/\",\"https://help.databricks.com\",\"https://databricks.com/try-databricks\",\"https://help.databricks.com/s/\",\"https://docs.databricks.com\",\"https://kb.databricks.com/\",\"http://docs.databricks.com/getting-started/index.html\",\"http://docs.databricks.com/introduction/index.html\",\"http://docs.databricks.com/getting-started/tutorials/index.html\",\"http://docs.databricks.com/release-notes/index.html\",\"http://docs.databricks.com/ingestion/index.html\",\"http://docs.databricks.com/exploratory-data-analysis/index.html\",\"http://docs.databricks.com/data-preparation/index.html\",\"http://docs.databricks.com/data-sharing/index.html\",\"http://docs.databricks.com/marketplace/index.html\",\"http://docs.databricks.com/workspace-index.html\",\"http://docs.databricks.com/machine-learning/index.html\",\"http://docs.databricks.com/sql/index.html\",\"http://docs.databricks.com/delta/index.html\",\"http://docs.databricks.com/dev-tools/index.html\",\"http://docs.databricks.com/integrations/index.html\",\"http://docs.databricks.com/administration-guide/index.html\",\"http://docs.databricks.com/security/index.html\",\"http://docs.databricks.com/data-governance/index.html\",\"http://docs.databricks.com/lakehouse-architecture/index.html\",\"http://docs.databricks.com/reference/api.html\",\"http://docs.databricks.com/resources/index.html\",\"http://docs.databricks.com/whats-coming.html\",\"http://docs.databricks.com/archive/index.html\",\"http://docs.databricks.com/lakehouse/index.html\",\"http://docs.databricks.com/getting-started/quick-start.html\",\"http://docs.databricks.com/getting-started/etl-quick-start.html\",\"http://docs.databricks.com/getting-started/lakehouse-e2e.html\",\"http://docs.databricks.com/getting-started/free-training.html\",\"http://docs.databricks.com/sql/language-manual/index.html\",\"http://docs.databricks.com/error-messages/index.html\",\"http://www.apache.org/\",\"https://databricks.com/privacy-policy\",\"https://databricks.com/terms-of-use\"]\n",
    "\n",
    "loader1 = WebBaseLoader(web_links)\n",
    "documents = loader1.load()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATING CHINKS USING TEXT SPLITTERS\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000 , chunk_overlap=20)\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "split_PDF = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING EMBEDDINGS\n",
    "\n",
    "Criando embeddings e armazenando in a vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/anaconda3/envs/figas2/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\" : \"cuda\"}\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name = model_name , model_kwargs = model_kwargs)\n",
    "\n",
    "#Armazenando no armazem de vetores\n",
    "\n",
    "vectorstore = FAISS.from_documents(all_splits , embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INICIANDO A CADEIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2217713/4099786668.py:9: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  result = chain({\"question\" : query1 , \"chat_history\" : chat_hist})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Articles about lakehouse architecture \n",
      "\n",
      "The scope of the lakehouse \n",
      "The first step to designing your data architecture with the Databricks Data Intelligence Platform is understanding its building blocks and how they would integrate with your systems. See The scope of the lakehouse platform.\n",
      "\n",
      "\n",
      "Guiding principles for the lakehouse \n",
      "Ground rules that define and influence your architecture. They explain the vision behind a lakehouse implementation and form the basis for future decisions on your data, analytics, and AI architecture. See Guiding principles for the lakehouse.\n",
      "\n",
      "\n",
      "Downloadable lakehouse reference architectures \n",
      "Downloadable architecture blueprints outline the recommended setup of the Databricks Data Intelligence Platform and its integration with cloud providers’ services. For reference architecture PDFs in 11 x 17 (A3) format, see Download lakehouse reference architectures.\n",
      "\n",
      "Data warehousing in your lakehouse \n",
      "The lakehouse architecture and Databricks SQL bring cloud data warehousing capabilities to your data lakes. Using familiar data structures, relations, and management tools, you can model a highly-performant, cost-effective data warehouse that runs directly on your data lake. For more information, see What is a data lakehouse?\n",
      "\n",
      "Data serving \n",
      "The final layer serves clean, enriched data to end users. The final tables should be designed to serve data for all your use cases. A unified governance model means you can track data lineage back to your single source of truth. Data layouts, optimized for different tasks, allow end users to access data for machine learning applications, data engineering, and business intelligence and reporting.\n",
      "To learn more about Delta Lake, see What is Delta Lake?\n",
      "To learn more about Unity Catalog, see What is Unity Catalog?\n",
      "\n",
      "\n",
      "\n",
      "Capabilities of a Databricks lakehouse \n",
      "A lakehouse built on Databricks replaces the current dependency on data lakes and data warehouses for modern data companies. Some key tasks you can perform include:\n",
      "\n",
      "How does the Databricks lakehouse work? \n",
      "Databricks is built on Apache Spark. Apache Spark enables a massively scalable engine that runs on compute resources decoupled from storage. For more information, see Apache Spark on Databricks\n",
      "The Databricks lakehouse uses two additional key technologies:\n",
      "\n",
      "Delta Lake: an optimized storage layer that supports ACID transactions and schema enforcement.\n",
      "Unity Catalog: a unified, fine-grained governance solution for data and AI.\n",
      "\n",
      "Question: What is Data lakehouse architecture in Databricks?\n",
      "Helpful Answer: Data lakehouse architecture in Databricks refers to the design and structure of a data storage system that combines the benefits of data lakes and data warehouses. It is built on top of Apache Spark and utilizes Delta Lake and Unity Catalog for storage and governance. This architecture allows for scalability, performance, and ease of use while providing a single source of truth for data.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(llm , vectorstore.as_retriever() , return_source_documents=True)\n",
    "\n",
    "\n",
    "chat_hist = []\n",
    "\n",
    "query1 = \"What is Data lakehouse architecture in Databricks?\"\n",
    "result = chain({\"question\" : query1 , \"chat_history\" : chat_hist})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'http://docs.databricks.com/lakehouse-architecture/index.html', 'title': 'Introduction to the well-architected data lakehouse | Databricks on AWS', 'description': 'Introduction to articles that describe principles and best practices for the implementation and operation of the Databricks lakehouse.', 'language': 'en-US'}, page_content='Articles about lakehouse architecture \\n\\nThe scope of the lakehouse \\nThe first step to designing your data architecture with the Databricks Data Intelligence Platform is understanding its building blocks and how they would integrate with your systems. See The scope of the lakehouse platform.\\n\\n\\nGuiding principles for the lakehouse \\nGround rules that define and influence your architecture. They explain the vision behind a lakehouse implementation and form the basis for future decisions on your data, analytics, and AI architecture. See Guiding principles for the lakehouse.\\n\\n\\nDownloadable lakehouse reference architectures \\nDownloadable architecture blueprints outline the recommended setup of the Databricks Data Intelligence Platform and its integration with cloud providers’ services. For reference architecture PDFs in 11 x 17 (A3) format, see Download lakehouse reference architectures.'), Document(metadata={'source': 'http://docs.databricks.com/sql/index.html', 'title': 'What is data warehousing on Databricks? | Databricks on AWS', 'description': 'Learn about building a data warehousing solution on the Databricks Platform using Databricks SQL.', 'language': 'en-US'}, page_content='Data warehousing in your lakehouse \\nThe lakehouse architecture and Databricks SQL bring cloud data warehousing capabilities to your data lakes. Using familiar data structures, relations, and management tools, you can model a highly-performant, cost-effective data warehouse that runs directly on your data lake. For more information, see What is a data lakehouse?'), Document(metadata={'source': 'http://docs.databricks.com/lakehouse/index.html', 'title': 'What is a data lakehouse? | Databricks on AWS', 'description': 'Use Databricks in a data lakehouse paradigm for generative AI, ACID transactions, data governance, ETL, BI, and machine learning.', 'language': 'en-US'}, page_content='Data serving \\nThe final layer serves clean, enriched data to end users. The final tables should be designed to serve data for all your use cases. A unified governance model means you can track data lineage back to your single source of truth. Data layouts, optimized for different tasks, allow end users to access data for machine learning applications, data engineering, and business intelligence and reporting.\\nTo learn more about Delta Lake, see What is Delta Lake?\\nTo learn more about Unity Catalog, see What is Unity Catalog?\\n\\n\\n\\nCapabilities of a Databricks lakehouse \\nA lakehouse built on Databricks replaces the current dependency on data lakes and data warehouses for modern data companies. Some key tasks you can perform include:'), Document(metadata={'source': 'http://docs.databricks.com/lakehouse/index.html', 'title': 'What is a data lakehouse? | Databricks on AWS', 'description': 'Use Databricks in a data lakehouse paradigm for generative AI, ACID transactions, data governance, ETL, BI, and machine learning.', 'language': 'en-US'}, page_content='How does the Databricks lakehouse work? \\nDatabricks is built on Apache Spark. Apache Spark enables a massively scalable engine that runs on compute resources decoupled from storage. For more information, see Apache Spark on Databricks\\nThe Databricks lakehouse uses two additional key technologies:\\n\\nDelta Lake: an optimized storage layer that supports ACID transactions and schema enforcement.\\nUnity Catalog: a unified, fine-grained governance solution for data and AI.')]\n"
     ]
    }
   ],
   "source": [
    "print(result['source_documents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGORA PARA O PDF\n",
    "\n",
    "Vamos lá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore2 = FAISS.from_documents(split_PDF , embeddings)\n",
    "\n",
    "chain2 = ConversationalRetrievalChain.from_llm(llm , vectorstore2.as_retriever() , return_source_documents=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "smaller than its deterministic counterpart, or its functioning may be easier to\n",
      "understand. Nondeterminism in ﬁnite automata is also a good introduction\n",
      "to nondeterminism in more powerful computational models because ﬁnite au-\n",
      "tomata are especially easy to understand. Now we turn to several examples of\n",
      "NFAs.\n",
      "Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.\n",
      "\n",
      "unique way from the preceding step. When the machine is in a given state and\n",
      "reads the next input symbol, we know what the next state will be—it is deter-\n",
      "mined. We call this deterministic computation. In a nondeterministic machine,\n",
      "several choices may exist for the next state at any point.\n",
      "Nondeterminism is a generalization of determinism, so every deterministic\n",
      "ﬁnite automaton is automatically a nondeterministic ﬁnite automaton. As Fig-\n",
      "ure 1.27 shows, nondeterministic ﬁnite automata may have additional features.\n",
      "Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.\n",
      "\n",
      "PART ONE\n",
      "AUTOMATA AND LANGUAGES\n",
      "Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.\n",
      "\n",
      "alphabet Σwe write Σεto be Σ∪{ε}.N o ww ec a nw r i t et h ef o r m a ld e s c r i p t i o n\n",
      "of the type of the transition function in an NFAasδ:Q×Σε−→ P(Q).\n",
      "DEFINITION 1.37\n",
      "Anondeterministic ﬁnite automaton is a 5-tuple (Q,Σ,δ ,q 0,F),\n",
      "where\n",
      "1.Qis a ﬁnite set of states,\n",
      "2.Σis a ﬁnite alphabet,\n",
      "3.δ:Q×Σε−→ P(Q)is the transition function,\n",
      "4.q0∈Qis the start state, and\n",
      "5.F⊆Qis the set of accept states.\n",
      "Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.\n",
      "\n",
      "Question: Describe a nonderteministic automata for me?\n",
      "Helpful Answer: Sure! A nondeterministic finite automaton (NFA) is a 5-tuple (Q,Σ,δ,q 0,F) where Q is a finite set of states, Σ is a finite alphabet, δ:Q×Σε−→ P(Q) is the transition function, q 0 is the start state, and F is the set of accept states. Unlike a deterministic finite automaton (DFA), which has only one possible next state for each current state and input symbol, an NFA can have multiple possible next states for each current state and input symbol. This means that the machine can \"guess\" which path to take next, rather than having a single predetermined path. Does that help clarify things?\n",
      "[Document(metadata={'source': '/home/rafael/Desktop/LFA/LFA2.pdf', 'page': 73}, page_content='smaller than its deterministic counterpart, or its functioning may be easier to\\nunderstand. Nondeterminism in ﬁnite automata is also a good introduction\\nto nondeterminism in more powerful computational models because ﬁnite au-\\ntomata are especially easy to understand. Now we turn to several examples of\\nNFAs.\\nCopyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.'), Document(metadata={'source': '/home/rafael/Desktop/LFA/LFA2.pdf', 'page': 70}, page_content='unique way from the preceding step. When the machine is in a given state and\\nreads the next input symbol, we know what the next state will be—it is deter-\\nmined. We call this deterministic computation. In a nondeterministic machine,\\nseveral choices may exist for the next state at any point.\\nNondeterminism is a generalization of determinism, so every deterministic\\nﬁnite automaton is automatically a nondeterministic ﬁnite automaton. As Fig-\\nure 1.27 shows, nondeterministic ﬁnite automata may have additional features.\\nCopyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.'), Document(metadata={'source': '/home/rafael/Desktop/LFA/LFA2.pdf', 'page': 52}, page_content='PART ONE\\nAUTOMATA AND LANGUAGES\\nCopyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.'), Document(metadata={'source': '/home/rafael/Desktop/LFA/LFA2.pdf', 'page': 76}, page_content='alphabet Σwe write Σεto be Σ∪{ε}.N o ww ec a nw r i t et h ef o r m a ld e s c r i p t i o n\\nof the type of the transition function in an NFAasδ:Q×Σε−→ P(Q).\\nDEFINITION 1.37\\nAnondeterministic ﬁnite automaton is a 5-tuple (Q,Σ,δ ,q 0,F),\\nwhere\\n1.Qis a ﬁnite set of states,\\n2.Σis a ﬁnite alphabet,\\n3.δ:Q×Σε−→ P(Q)is the transition function,\\n4.q0∈Qis the start state, and\\n5.F⊆Qis the set of accept states.\\nCopyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.')]\n"
     ]
    }
   ],
   "source": [
    "chat2_hist = []\n",
    "\n",
    "query3 = \"Describe a nonderteministic automata for me?\"\n",
    "result2 = chain2({\"question\" : query3 , \"chat_history\" : chat2_hist})\n",
    "\n",
    "print(result2['answer'])\n",
    "\n",
    "\n",
    "print(result2['source_documents'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "figas2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
